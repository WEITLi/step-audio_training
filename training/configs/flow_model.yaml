# Step-Audio-EditX Flow Model Training Configuration
# Based on CosyVoice architecture, adapted for dual-codebook tokens

# Random seed for reproducibility
__set_seed1: !apply:random.seed [2026]
__set_seed2: !apply:numpy.random.seed [2026]
__set_seed3: !apply:torch.manual_seed [2026]
__set_seed4: !apply:torch.cuda.manual_seed_all [2026]

# Audio parameters
sample_rate: 24000  # Step-Audio-EditX uses 24kHz
hop_size: 480       # 24000 / 50 = 480 (50Hz frame rate)

# Model parameters
spk_embed_dim: 192  # Speaker embedding dimension (from campplus)
flow_input_size: 512
flow_output_size: 80  # Mel spectrogram bins

# Token parameters
# Step-Audio-EditX uses dual-codebook: VQ02 (1024) + VQ06 (1024)
vq02_vocab_size: 1024
vq06_vocab_size: 1024
total_vocab_size: 2048  # Combined vocab
input_frame_rate: 48    # Adjust based on your token rate

# Flow Model - CausalMaskedDiffWithXvec (for streaming support)
flow: !new:cosyvoice.flow.flow.CausalMaskedDiffWithXvec
    input_size: !ref <flow_input_size>
    output_size: !ref <flow_output_size>
    spk_embed_dim: !ref <spk_embed_dim>
    output_type: 'mel'
    vocab_size: !ref <total_vocab_size>
    input_frame_rate: !ref <input_frame_rate>
    only_mask_loss: True
    token_mel_ratio: 2
    pre_lookahead_len: 3
    
    # Encoder: Conformer for token encoding
    encoder: !new:cosyvoice.transformer.encoder.ConformerEncoder
        input_size: !ref <flow_input_size>
        output_size: 512
        attention_heads: 8
        linear_units: 2048
        num_blocks: 6
        dropout_rate: 0.1
        positional_dropout_rate: 0.1
        attention_dropout_rate: 0.1
        normalize_before: True
        input_layer: 'linear'
        pos_enc_layer_type: 'rel_pos_espnet'
        selfattention_layer_type: 'rel_selfattn'
        use_cnn_module: False
        macaron_style: False
        # Causal settings for streaming
        use_dynamic_chunk: True
        use_dynamic_left_chunk: False
        static_chunk_size: 16  # Chunk size for streaming
    
    # Decoder: Conditional Flow Matching
    decoder: !new:cosyvoice.flow.flow_matching.ConditionalCFM
        in_channels: 240
        n_spks: 1
        spk_emb_dim: 80
        cfm_params: !new:omegaconf.DictConfig
            content:
                sigma_min: 1e-06
                solver: 'euler'
                t_scheduler: 'cosine'
                training_cfg_rate: 0.2
                inference_cfg_rate: 0.7
                reg_loss_type: 'l1'
        estimator: !new:cosyvoice.flow.decoder.ConditionalDecoder
            in_channels: 320
            out_channels: 80
            channels: [256, 256]
            dropout: 0.0
            attention_head_dim: 64
            n_blocks: 4
            num_mid_blocks: 12
            num_heads: 8
            act_fn: 'gelu'

# Mel spectrogram configuration
mel_feat_conf:
    n_fft: 2048
    num_mels: 80
    sampling_rate: !ref <sample_rate>
    hop_size: !ref <hop_size>
    win_size: 2048
    fmin: 0
    fmax: 12000
    center: False

# Data processing functions
parquet_opener: !name:cosyvoice.dataset.processor.parquet_opener

# Custom tokenizer for Step-Audio-EditX dual-codebook
# You'll need to implement this based on your tokenizer
get_tokenizer: !name:step_audio_tokenizer.get_tokenizer
    tokenizer_path: 'pretrained_models/Step-Audio-Tokenizer'

tokenize: !name:cosyvoice.dataset.processor.tokenize
    get_tokenizer: !ref <get_tokenizer>
    allowed_special: 'all'

filter: !name:cosyvoice.dataset.processor.filter
    max_length: 40960   # Max audio length in samples
    min_length: 4800    # Min audio length (0.2s at 24kHz)
    token_max_length: 500
    token_min_length: 10

resample: !name:cosyvoice.dataset.processor.resample
    resample_rate: !ref <sample_rate>

# Mel feature extraction
feat_extractor: !name:matcha.utils.audio.mel_spectrogram
    n_fft: !ref <mel_feat_conf[n_fft]>
    num_mels: !ref <mel_feat_conf[num_mels]>
    sampling_rate: !ref <sample_rate>
    hop_size: !ref <hop_size>
    win_size: !ref <mel_feat_conf[win_size]>
    fmin: !ref <mel_feat_conf[fmin]>
    fmax: !ref <mel_feat_conf[fmax]>
    center: False

compute_fbank: !name:cosyvoice.dataset.processor.compute_fbank
    feat_extractor: !ref <feat_extractor>

parse_embedding: !name:cosyvoice.dataset.processor.parse_embedding
    normalize: True

shuffle: !name:cosyvoice.dataset.processor.shuffle
    shuffle_size: 1000

sort: !name:cosyvoice.dataset.processor.sort
    sort_size: 500

batch: !name:cosyvoice.dataset.processor.batch
    batch_type: 'dynamic'
    max_frames_in_batch: 2000  # Adjust based on GPU memory

padding: !name:cosyvoice.dataset.processor.padding
    use_spk_embedding: True

# Data pipeline
data_pipeline: [
    !ref <parquet_opener>,
    !ref <tokenize>,
    !ref <filter>,
    !ref <resample>,
    !ref <compute_fbank>,
    !ref <parse_embedding>,
    !ref <shuffle>,
    !ref <sort>,
    !ref <batch>,
    !ref <padding>,
]

# Training configuration
train_conf:
    # Optimizer
    optim: adam
    optim_conf:
        lr: 0.001  # Initial learning rate for pretraining
                   # Use 1e-5 for fine-tuning
        betas: [0.9, 0.999]
        eps: 1e-8
        weight_decay: 0.0
    
    # Learning rate scheduler
    scheduler: warmuplr  # Use 'constantlr' for fine-tuning
    scheduler_conf:
        warmup_steps: 2500
    
    # Training settings
    max_epoch: 100
    grad_clip: 5.0
    accum_grad: 2  # Gradient accumulation steps
    log_interval: 100
    save_per_step: 5000  # Save checkpoint every N steps
    
    # Validation
    val_interval: 1  # Validate every N epochs
    
    # Early stopping (optional)
    patience: 10  # Stop if no improvement for N epochs
