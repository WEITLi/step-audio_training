# 基础配置
basic:
  seed: 42
  device: "cuda"
  max_epoch: 50
  log_interval: 50  # 每50步打印日志
  val_interval: 1   # 每1个epoch验证一次

# 模型配置（适配StepAudioEditX的LLM/Flow）
model:
  # LLM配置（轻量化LoRA微调）
  llm:
    model_path: "./pretrained_models/Step-Audio-EditX"  # StepAudioEditX原有LLM权重路径
    lora: True
    lora_r: 8
    lora_alpha: 32
    lora_dropout: 0.05
    lora_target_modules: ["q_proj", "v_proj", "k_proj"]  # LLM注意力层
    freeze_backbone: True  # 仅训练LoRA层，冻结LLM主干
  
  # Flow配置（仅微调解码器）
  flow:
    model_path: "./pretrained_models/Step-Audio-EditX/CosyVoice-300M-25Hz"  # Flow权重路径
    freeze_encoder: True  # 冻结Flow编码器
    freeze_vocoder: True  # 冻结BigVGAN声码器（仅微调Flow解码器）
    n_timesteps: 10       # 与StepAudioEditX推理时的生成步数一致

# 优化器配置
optim:
  lr_llm: 1e-4          # LLM LoRA学习率
  lr_flow: 1e-5         # Flow解码器学习率
  warmup_steps: 3000
  weight_decay: 0.01
  grad_clip: 5.0        # 梯度裁剪阈值
  accum_grad: 4         # 梯度累积（降低显存占用）

# 数据集配置（适配语音克隆/编辑任务）
data:
  train_path: "./data/train.jsonl"  # 训练集路径
  val_path: "./data/val.jsonl"      # 验证集路径
  batch_size: 2                     # 批量大小（根据显存调整）
  sample_rate: 24000                # 与StepAudioEditX输出采样率一致
  max_audio_len: 10                 # 音频最大长度（秒）
  max_text_len: 200                 # 文本最大长度（token数）
  num_workers: 4                    # 数据加载线程数

# 保存配置
save:
  ckpt_dir: "./ckpt/finetune"       # 微调权重保存目录
  save_best: True                   # 保存验证集损失最低的权重
  save_interval: 5                  # 每5个epoch保存一次 checkpoint

# 损失权重
loss:
  llm_weight: 0.4                   # LLM损失权重
  flow_weight: 0.6                  # Flow损失权重

# 阶段训练配置
stage:
  stage1_epochs: 25                 # 阶段1：单独微调LLM的epoch数
  stage2_epochs: 25                 # 阶段2：单独微调Flow的epoch数
  stage3_epochs: 5                  # 阶段3：联合微调的epoch数（可选）
  joint_lr_scale: 0.1               # 联合微调时学习率缩放因子
